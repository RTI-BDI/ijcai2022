How to correctly use an offline defined simulation to create N versions, test them and compare their outcomes
---------------------------------------------------------------------------------------------

1. Store the offline designed simulation in folder "../simulations/{user}/" (the path refers to the current location of this file)
 - {user} is a variable. It is set by default to: francesco.
 - If a different name is needed, create a folder in "../simulations/{new_name}" and replace the value 'francesco' with the new name.
 - NOTE: If the name is changed, it must also be replaced in file: "random_events_generator_network.ned", "compare_simulations_results.ned", "analyze_simulations.ned" and "ag_network.ned"

2. Use the tool to generate N simulations derived from the designed one:
 a. Using the IDE OMNET++: go to file 'random_events_generator_network.ned' and replace the value for the parameter 'simulation_name' with the name of your simulation
 b. Using the terminal: use the following command to setup the parameter needed (the command must be executed from inside the kronosim folder location)
 	./kronosim -u Cmdenv -c random_events_generator omnetpp.ini 
	--random_events_generator_network.simulation.folder_path='"{path to where your simulation is}"'  
	--random_events_generator_network.simulation.destination_folder='"{path to where the simulations will be stored}"'  
	--random_events_generator_network.simulation.simulation_name='"{your simulation name}"' 
	--random_events_generator_network.simulation.convertFile=false
	--random_events_generator_network.simulation.sched_type=$SCHED_TYPE 		// (0 = FCFS, 1 = RR, 2 = EDF)
	--random_events_generator_network.simulation.logger_level=$LOGGER_LEVEL	// 0 = PrintNothing, Error, Debug, Default, Warning, 5 = EssentialInfo, EveryInfo, EveryInfoPlus 
	
 - Note: destination_folder is set to '/generated_simulations/' by default (../simulations/{user}/generated_simulations/). 
   If the folder is not present, it gets created (as long as the user has the right permissions).
 - In 'generated_simulations' each simulation will have a dedicated folder. Inside the folder the program will store the file needed by each of the N generated simulations.
 - The command has been split into multiple lines for the sake of visualization. It must be executed as a single line.
 

3. Run all the N simulations (each simulation will have the name: {simulation_name}_seed_n, where n goes from 1 to N):
 a. Using the IDE OMNET++: go to file 'ag_network.ned', replace the parameter 'simulation_name' with the current seed version and execute. This process must be repeated N times.
 b. Using the terminal: move to location "../simulations/{user}/generated_simulations/{simulation_name}/" and execute file 'fast_execution_cmd.sh' (use: bash fast_execution_cmd.sh)
    This file has been generated when performing step 2.
    Note: the .sh file sequentially execute on every generated seed simulation .

----------------------------------------------------------------------------------------------

4. When all the seeds have been generated and executed, you can compute the average metrics.
 a. If you used the terminal to run 'fast_execution_cmd.sh', then this step has already been performed.
 b. Otherwise, go to 'analyze_simulations_network.ned', set parameter 'simulation_name = {your simulation name}' and execute the file. 
 - The results are stored in a file called '{scheduling_algorithm}_simulation_analysis.json' where {scheduling_algorithm} is the algorithm you chose between EDF, FCFS and RR.
 - The file combines the metrics of each seed simulation. The aim is to have more general idea of the performance of your scenario.

5. You can use 'compare_simulations_results_network.ned' to study each simulation by comparing the results generated by the three algorithm.
 - To do so, set 'single_simulation = true' and replace the value of variable 'simulation_name' with the name of your simulation.
 - This step takes the EDF_simulation_analysis.json, RR_simulation_analysis.json and FCFS_simulation_analysis.json and compare them. 
 For instance, it compares the number of deadline miss, the executed tasks, the achieved goals and more.
